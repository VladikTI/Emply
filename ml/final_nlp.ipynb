{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8b9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импорт данных\n",
    "import os, json\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "CV = CountVectorizer()\n",
    "final_df = pd.DataFrame()\n",
    "ank1 = open('анкета1.json',encoding=\"utf-8\")\n",
    "ank2 = open('anketa.json', encoding=\"utf-8\")\n",
    "ank1 = json.load(ank1)\n",
    "ank2 = json.load(ank2)\n",
    "ank1 = pd.DataFrame.from_dict(pd.json_normalize(ank1), orient = \"columns\")\n",
    "ank2 = pd.DataFrame.from_dict(pd.json_normalize(ank2), orient = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77bb1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 76.84374773095676 %\n"
     ]
    }
   ],
   "source": [
    "#Единая функция обработки и результата программы\n",
    "import nest_asyncio\n",
    "import math\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)\n",
    "start_time = time.time()\n",
    "async def program(ank1, ank2):\n",
    "    ank1 = ank1.fillna(value=\"\")\n",
    "    ank2 = ank2.fillna(value=\"\")\n",
    "    ank1 = ank1.applymap(str)\n",
    "    ank2 = ank2.applymap(str)\n",
    "    ank1 = ank1.applymap(lambda x: word_tokenize(x))\n",
    "    ank2 = ank2.applymap(lambda x: word_tokenize(x))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    ank1=ank1.applymap(str)\n",
    "    ank2=ank2.applymap(str)\n",
    "    ank1['Опыт работы'] = ank1['Опыт работы'].map(tokenizer.tokenize)\n",
    "    ank1['Образование'] = ank1['Образование'].map(tokenizer.tokenize)\n",
    "    ank1['Проекты'] = ank1['Проекты'].map(tokenizer.tokenize)\n",
    "    ank1['Дополнительная информация'] = ank1['Дополнительная информация'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Язык программирования'] = ank1['Навыки.Язык программирования'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Фреймворки'] = ank1['Навыки.Фреймворки'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Базы данных'] = ank1['Навыки.Базы данных'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Работа с API'] = ank1['Навыки.Работа с API'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Инструменты'] = ank1['Навыки.Инструменты'].map(tokenizer.tokenize)\n",
    "    ank1['Языки.Русский'] = ank1['Языки.Русский'].map(tokenizer.tokenize)\n",
    "    ank1['Языки.Английский'] = ank1['Языки.Английский'].map(tokenizer.tokenize)\n",
    "    ank2['Опыт работы'] = ank2['Опыт работы'].map(tokenizer.tokenize)\n",
    "    ank2['Образование'] = ank2['Образование'].map(tokenizer.tokenize)\n",
    "    ank2['Проекты'] = ank2['Проекты'].map(tokenizer.tokenize)\n",
    "    ank2['Дополнительная информация'] = ank2['Дополнительная информация'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Язык программирования'] = ank2['Навыки.Язык программирования'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Фреймворки'] = ank2['Навыки.Фреймворки'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Базы данных'] = ank2['Навыки.Базы данных'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Работа с API'] = ank2['Навыки.Работа с API'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Инструменты'] = ank2['Навыки.Инструменты'].map(tokenizer.tokenize)\n",
    "    ank2['Языки.Русский'] = ank2['Языки.Русский'].map(tokenizer.tokenize)\n",
    "    ank2['Языки.Английский'] = ank2['Языки.Английский'].map(tokenizer.tokenize)\n",
    "    ank1['Опыт работы'] = ank1['Опыт работы'].astype(str).str.lower()\n",
    "    ank1['Образование'] = ank1['Образование'].astype(str).str.lower()\n",
    "    ank1['Проекты'] = ank1['Проекты'].astype(str).str.lower()\n",
    "    ank1['Дополнительная информация'] = ank1['Дополнительная информация'].astype(str).str.lower()\n",
    "    ank1['Навыки.Язык программирования'] = ank1['Навыки.Язык программирования'].astype(str).str.lower()\n",
    "    ank1['Навыки.Фреймворки'] = ank1['Навыки.Фреймворки'].astype(str).str.lower()\n",
    "    ank1['Навыки.Базы данных'] = ank1['Навыки.Базы данных'].astype(str).str.lower()\n",
    "    ank1['Навыки.Работа с API'] = ank1['Навыки.Работа с API'].astype(str).str.lower()\n",
    "    ank1['Навыки.Инструменты'] = ank1['Навыки.Инструменты'].astype(str).str.lower()\n",
    "    ank1['Языки.Русский'] = ank1['Языки.Русский'].astype(str).str.lower()\n",
    "    ank1['Языки.Английский'] = ank1['Языки.Английский'].astype(str).str.lower()\n",
    "    ank2['Опыт работы'] = ank2['Опыт работы'].astype(str).str.lower()\n",
    "    ank2['Образование'] = ank2['Образование'].astype(str).str.lower()\n",
    "    ank2['Проекты'] = ank2['Проекты'].astype(str).str.lower()\n",
    "    ank2['Дополнительная информация'] = ank2['Дополнительная информация'].astype(str).str.lower()\n",
    "    ank2['Навыки.Язык программирования'] = ank2['Навыки.Язык программирования'].astype(str).str.lower()\n",
    "    ank2['Навыки.Фреймворки'] = ank2['Навыки.Фреймворки'].astype(str).str.lower()\n",
    "    ank2['Навыки.Базы данных'] = ank2['Навыки.Базы данных'].astype(str).str.lower()\n",
    "    ank2['Навыки.Работа с API'] = ank2['Навыки.Работа с API'].astype(str).str.lower()\n",
    "    ank2['Навыки.Инструменты'] = ank2['Навыки.Инструменты'].astype(str).str.lower()\n",
    "    ank2['Языки.Русский'] = ank2['Языки.Русский'].astype(str).str.lower()\n",
    "    ank2['Языки.Английский'] = ank2['Языки.Английский'].astype(str).str.lower()\n",
    "    ank1['soup'] = ank1.apply(lambda row: row['Опыт работы'] + row['Образование'] + row['Проекты'] + row['Дополнительная информация'] + row['Навыки.Язык программирования'] + row['Навыки.Фреймворки'] + row['Навыки.Базы данных'] + row['Навыки.Работа с API'] + row['Навыки.Инструменты'] + row['Языки.Русский'] + row['Языки.Английский'], axis=1)\n",
    "    ank2['soup'] = ank2.apply(lambda row: row['Опыт работы'] + row['Образование'] + row['Проекты'] + row['Дополнительная информация'] + row['Навыки.Язык программирования'] + row['Навыки.Фреймворки'] + row['Навыки.Базы данных'] + row['Навыки.Работа с API'] + row['Навыки.Инструменты'] + row['Языки.Русский'] + row['Языки.Английский'], axis=1)\n",
    "    cosine_matrix = CV.fit_transform(ank1['soup'])\n",
    "    cosine_matrix2 = CV.transform(ank2['soup'])\n",
    "    cosine_sim = cosine_similarity(cosine_matrix, cosine_matrix2)\n",
    "    cosine_sim = cosine_sim.flatten()\n",
    "    perc_dist = cosine_sim * 100\n",
    "    final_df['similarities'] = pd.Series(cosine_sim)\n",
    "    final_df['percentage'] = pd.Series(perc_dist)\n",
    "    final_df['name'] = ank1['Имя']\n",
    "    final_df['surname'] = ank1['Фамилия']\n",
    "    final_df['email'] = ank1['Контактная информация.Электронная почта']\n",
    "    return 'Similarity:' + ' ' + str(final_df['percentage'].item())+ ' ' + '%'\n",
    "print(loop.run_until_complete(program(ank1, ank2)))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfc18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client(email):\n",
    "    if (final_df['percentage'] > 0.7).any():\n",
    "        return 'Скорее всего вы пройдете! Ура!'\n",
    "    elif (final_df['percentage']<0.7).any() and (final_df['percentage'] >= 0.5).any():\n",
    "        return 'Ваши шансы высоки!'\n",
    "    elif (final_df['percentage']<0.5).any() and (final_df['percentage'] >= 0.3).any():\n",
    "        return 'Возможно вы пройдете дальше!'\n",
    "    else:\n",
    "        return 'Сожалеем, но скорее всего вы не пройдете на следующий этап('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c7753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6308e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee672f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score:[[99.99998808]]%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sholo\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)\n",
    "\n",
    "async def rubert(ank1, ank2):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    ank1 = ank1.fillna(value=\"\")\n",
    "    ank2 = ank2.fillna(value=\"\")\n",
    "    ank1 = ank1.applymap(str)\n",
    "    ank2 = ank2.applymap(str)\n",
    "    ank1 = ank1.applymap(lambda x: word_tokenize(x))\n",
    "    ank2 = ank2.applymap(lambda x: word_tokenize(x))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')#first tokenizer\n",
    "    model = BertModel.from_pretrained(\"cointegrated/rubert-tiny2\")#rubert model\n",
    "    #data preprocessing\n",
    "    ank1=ank1.applymap(str)\n",
    "    ank2=ank2.applymap(str)\n",
    "    ank1['Опыт работы'] = ank1['Опыт работы'].map(tokenizer.tokenize)\n",
    "    ank1['Образование'] = ank1['Образование'].map(tokenizer.tokenize)\n",
    "    ank1['Проекты'] = ank1['Проекты'].map(tokenizer.tokenize)\n",
    "    ank1['Дополнительная информация'] = ank1['Дополнительная информация'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Язык программирования'] = ank1['Навыки.Язык программирования'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Фреймворки'] = ank1['Навыки.Фреймворки'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Базы данных'] = ank1['Навыки.Базы данных'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Работа с API'] = ank1['Навыки.Работа с API'].map(tokenizer.tokenize)\n",
    "    ank1['Навыки.Инструменты'] = ank1['Навыки.Инструменты'].map(tokenizer.tokenize)\n",
    "    ank1['Языки.Русский'] = ank1['Языки.Русский'].map(tokenizer.tokenize)\n",
    "    ank1['Языки.Английский'] = ank1['Языки.Английский'].map(tokenizer.tokenize)\n",
    "    ank2['Опыт работы'] = ank2['Опыт работы'].map(tokenizer.tokenize)\n",
    "    ank2['Образование'] = ank2['Образование'].map(tokenizer.tokenize)\n",
    "    ank2['Проекты'] = ank2['Проекты'].map(tokenizer.tokenize)\n",
    "    ank2['Дополнительная информация'] = ank2['Дополнительная информация'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Язык программирования'] = ank2['Навыки.Язык программирования'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Фреймворки'] = ank2['Навыки.Фреймворки'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Базы данных'] = ank2['Навыки.Базы данных'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Работа с API'] = ank2['Навыки.Работа с API'].map(tokenizer.tokenize)\n",
    "    ank2['Навыки.Инструменты'] = ank2['Навыки.Инструменты'].map(tokenizer.tokenize)\n",
    "    ank2['Языки.Русский'] = ank2['Языки.Русский'].map(tokenizer.tokenize)\n",
    "    ank2['Языки.Английский'] = ank2['Языки.Английский'].map(tokenizer.tokenize)\n",
    "    ank1['Опыт работы'] = ank1['Опыт работы'].astype(str).str.lower()\n",
    "    ank1['Образование'] = ank1['Образование'].astype(str).str.lower()\n",
    "    ank1['Проекты'] = ank1['Проекты'].astype(str).str.lower()\n",
    "    ank1['Дополнительная информация'] = ank1['Дополнительная информация'].astype(str).str.lower()\n",
    "    ank1['Навыки.Язык программирования'] = ank1['Навыки.Язык программирования'].astype(str).str.lower()\n",
    "    ank1['Навыки.Фреймворки'] = ank1['Навыки.Фреймворки'].astype(str).str.lower()\n",
    "    ank1['Навыки.Базы данных'] = ank1['Навыки.Базы данных'].astype(str).str.lower()\n",
    "    ank1['Навыки.Работа с API'] = ank1['Навыки.Работа с API'].astype(str).str.lower()\n",
    "    ank1['Навыки.Инструменты'] = ank1['Навыки.Инструменты'].astype(str).str.lower()\n",
    "    ank1['Языки.Русский'] = ank1['Языки.Русский'].astype(str).str.lower()\n",
    "    ank1['Языки.Английский'] = ank1['Языки.Английский'].astype(str).str.lower()\n",
    "    ank2['Опыт работы'] = ank2['Опыт работы'].astype(str).str.lower()\n",
    "    ank2['Образование'] = ank2['Образование'].astype(str).str.lower()\n",
    "    ank2['Проекты'] = ank2['Проекты'].astype(str).str.lower()\n",
    "    ank2['Дополнительная информация'] = ank2['Дополнительная информация'].astype(str).str.lower()\n",
    "    ank2['Навыки.Язык программирования'] = ank2['Навыки.Язык программирования'].astype(str).str.lower()\n",
    "    ank2['Навыки.Фреймворки'] = ank2['Навыки.Фреймворки'].astype(str).str.lower()\n",
    "    ank2['Навыки.Базы данных'] = ank2['Навыки.Базы данных'].astype(str).str.lower()\n",
    "    ank2['Навыки.Работа с API'] = ank2['Навыки.Работа с API'].astype(str).str.lower()\n",
    "    ank2['Навыки.Инструменты'] = ank2['Навыки.Инструменты'].astype(str).str.lower()\n",
    "    ank2['Языки.Русский'] = ank2['Языки.Русский'].astype(str).str.lower()\n",
    "    ank2['Языки.Английский'] = ank2['Языки.Английский'].astype(str).str.lower()\n",
    "    ank1['soup'] = ank1.apply(lambda row: row['Опыт работы'] + row['Образование'] + row['Проекты'] + row['Дополнительная информация'] + row['Навыки.Язык программирования'] + row['Навыки.Фреймворки'] + row['Навыки.Базы данных'] + row['Навыки.Работа с API'] + row['Навыки.Инструменты'] + row['Языки.Русский'] + row['Языки.Английский'], axis=1)\n",
    "    ank2['soup'] = ank2.apply(lambda row: row['Опыт работы'] + row['Образование'] + row['Проекты'] + row['Дополнительная информация'] + row['Навыки.Язык программирования'] + row['Навыки.Фреймворки'] + row['Навыки.Базы данных'] + row['Навыки.Работа с API'] + row['Навыки.Инструменты'] + row['Языки.Русский'] + row['Языки.Английский'], axis=1)\n",
    "    \n",
    "    tokenizer2 = BertTokenizer.from_pretrained('cointegrated/rubert-tiny')#bert tokenizer\n",
    "    ank1_string = str(ank1['soup'])\n",
    "    ank2_string = str(ank2['soup'])\n",
    "    \n",
    "    encoded_dict = tokenizer2.encode_plus(#encoding\n",
    "                    [ank1_string, ank2_string],                      # Sentence to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "               )\n",
    "    input_ids.append(encoded_dict['input_ids'])#list of input ids\n",
    "    attention_mask.append(encoded_dict['attention_mask'])#list of attention masks\n",
    "    input_id = torch.cat(input_ids, dim=1)#concating input ids tensors\n",
    "    attention_masks = torch.cat(attention_mask, dim=1)#concating tensors attention mask\n",
    "    output = model(**encoded_dict)#outputs\n",
    "    embeddings = output.last_hidden_state#embeddings\n",
    "    mask = encoded_dict['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()#mask\n",
    "    masked_embeddings = embeddings * mask#masked embeddings\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    counted = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    mean_pooled = summed / counted\n",
    "    mean_pooled = mean_pooled.detach().numpy()\n",
    "    scores = np.zeros((mean_pooled.shape[0], mean_pooled.shape[0]))#sim scores\n",
    "    for i in range(mean_pooled.shape[0]):\n",
    "        scores[i, :] = cosine_similarity(\n",
    "            [mean_pooled[i]],\n",
    "            mean_pooled\n",
    "        )[0]\n",
    "    perc_score = scores*100#percentage\n",
    "    \n",
    "    return 'Similarity score:' + str(perc_score) + '%'\n",
    "\n",
    "print(loop.run_until_complete(rubert(ank1, ank2)))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdbc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sholo\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Часть с рекомендациями- хуйня\n",
    "data = open('forms.json', encoding='utf-8')\n",
    "data = json.load(data)\n",
    "data = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n",
    "def get_recommendations(data):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    data = data.fillna(value=\"\")\n",
    "    data = data.applymap(str)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data['Опыт работы'] = data['Опыт работы'].map(tokenizer.tokenize)\n",
    "    data['Образование'] = data['Образование'].map(tokenizer.tokenize)\n",
    "    data['Проекты'] = data['Проекты'].map(tokenizer.tokenize)\n",
    "    data['Дополнительная информация'] = data['Дополнительная информация'].map(tokenizer.tokenize)\n",
    "    data['Навыки.Язык программирования'] = data['Навыки.Язык программирования'].map(tokenizer.tokenize)\n",
    "    data['Навыки.Фреймворки'] = data['Навыки.Фреймворки'].map(tokenizer.tokenize)\n",
    "    data['Навыки.Базы данных'] = data['Навыки.Базы данных'].map(tokenizer.tokenize)\n",
    "    data['Навыки.Работа с API'] = data['Навыки.Работа с API'].map(tokenizer.tokenize)\n",
    "    data['Навыки.Инструменты'] = data['Навыки.Инструменты'].map(tokenizer.tokenize)\n",
    "    data['Языки.Русский'] = data['Языки.Русский'].map(tokenizer.tokenize)\n",
    "    data['Языки.Английский'] = data['Языки.Английский'].map(tokenizer.tokenize)\n",
    "    data['Опыт работы'] = data['Опыт работы'].astype(str).str.lower()\n",
    "    data['Образование'] = data['Образование'].astype(str).str.lower()\n",
    "    data['Проекты'] = data['Проекты'].astype(str).str.lower()\n",
    "    data['Дополнительная информация'] = data['Дополнительная информация'].astype(str).str.lower()\n",
    "    data['Навыки.Язык программирования'] = data['Навыки.Язык программирования'].astype(str).str.lower()\n",
    "    data['Навыки.Фреймворки'] = data['Навыки.Фреймворки'].astype(str).str.lower()\n",
    "    data['Навыки.Базы данных'] = data['Навыки.Базы данных'].astype(str).str.lower()\n",
    "    data['Навыки.Работа с API'] = data['Навыки.Работа с API'].astype(str).str.lower()\n",
    "    data['Навыки.Инструменты'] = data['Навыки.Инструменты'].astype(str).str.lower()\n",
    "    data['Языки.Русский'] = data['Языки.Русский'].astype(str).str.lower()\n",
    "    data['Языки.Английский'] = data['Языки.Английский'].astype(str).str.lower()\n",
    "    data['soup'] = data.apply(lambda row: row['Опыт работы'] + row['Образование'] + row['Проекты'] + row['Дополнительная информация'] + row['Навыки.Язык программирования'] + row['Навыки.Фреймворки'] + row['Навыки.Базы данных'] + row['Навыки.Работа с API'] + row['Навыки.Инструменты'] + row['Языки.Русский'] + row['Языки.Английский'], axis=1)\n",
    "    \n",
    "    tokenizer2 = BertTokenizer.from_pretrained('cointegrated/rubert-tiny')#bert tokenizer\n",
    "    model = BertModel.from_pretrained(\"cointegrated/rubert-tiny2\")#rubert model\n",
    "    X = data.iloc[:306]\n",
    "    X = X.soup.values\n",
    "    y = data.iloc[[307]]\n",
    "    y = y.soup.values\n",
    "    X_string = str(X)\n",
    "    y_string = str(y)\n",
    "    for row in range(len(X_string)):\n",
    "        encoded_dict = tokenizer2.encode_plus(#encoding\n",
    "                    [X_string[row],y_string],                      # Sentence to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "               )\n",
    "        input_ids.append(encoded_dict['input_ids'])#list of input ids\n",
    "        attention_mask.append(encoded_dict['attention_mask'])#list of attention masks\n",
    "        input_id = torch.cat(input_ids, dim=1)#concating input ids tensors\n",
    "        attention_masks = torch.cat(attention_mask, dim=1)#concating tensors attention mask\n",
    "    output = model(**encoded_dict)#outputs\n",
    "    embeddings = output.last_hidden_state#embeddings\n",
    "    mask = encoded_dict['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()#mask\n",
    "    masked_embeddings = embeddings * mask#masked embeddings\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    counted = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    mean_pooled = summed / counted\n",
    "    mean_pooled = mean_pooled.detach().numpy()\n",
    "    scores = np.zeros((mean_pooled.shape[0], mean_pooled.shape[0]))#sim scores\n",
    "    for i in range(mean_pooled.shape[0]):\n",
    "        scores[i, :] = cosine_similarity(\n",
    "            [mean_pooled[i]],\n",
    "            mean_pooled\n",
    "        )[0]\n",
    "    perc_score = scores * 100\n",
    "    perc_score = perc_score.flatten()\n",
    "    data['similarities'] = pd.Series(perc_score)\n",
    "    ans = pd.DataFrame()\n",
    "    indices = pd.Series(X.index, index=X['Имя']).drop_duplicates()\n",
    "    sim_scores = list(enumerate(perc_score[indices]))\n",
    "    \n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar user forms\n",
    "    sim_scores = perc_score[1:11]\n",
    "    ans = pd.DataFrame(sim_scores, columns=['indexes', 'sim_scores'])\n",
    "    ans = ans.drop('indexes', axis=1)\n",
    "\n",
    "    # Get the user indices\n",
    "    user_indices = [i[0] for i in sim_scores]\n",
    "    ans['user_indices'] = user_indices\n",
    "    ans['names'] = X['Имя'].iloc[user_indices].values\n",
    "\n",
    "    # Return the top 10 most similar forms\n",
    "    print('Топ 10 кандидатов:')\n",
    "    return ans\n",
    "print(get_recommendations(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cf628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
